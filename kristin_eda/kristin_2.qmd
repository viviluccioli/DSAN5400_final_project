---
title: "Tone Analysis of Fox News Coverage"
author: "Kristin Lloyd"
format: 
  html:
    embed-resources: true
    code-fold: true
---

## ðŸ“¥ Data Import and Cleaning

```{python}
import pandas as pd
import glob

# Load all fox CSV files
csv_files = glob.glob("../data/fox/fox*.csv")
df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)

# Select relevant columns
columns_of_interest = [
    "parsed_date", "url", "headline_from_url",
    "V2Themes", "V2Locations", "V2Persons",
    "V2Organizations", "V2Tone"
]
df = df[columns_of_interest]

# Convert parsed_date to datetime
df["parsed_date"] = pd.to_datetime(df["parsed_date"], errors="coerce")

# Preview structure and missing values
df.info()
df.isnull().sum()
df.sample(5)

```

```{python}

# Split V2Tone into tone, positive_score, and negative_score
tone_split = df["V2Tone"].str.split(",", expand=True)
df["tone"] = pd.to_numeric(tone_split[0], errors="coerce")
df["positive_score"] = pd.to_numeric(tone_split[1], errors="coerce")
df["negative_score"] = pd.to_numeric(tone_split[2], errors="coerce")

# Descriptive stats
df["tone"].describe()
df["positive_score"].describe()
df["negative_score"].describe()

```

```{python}

import matplotlib.pyplot as plt

# Create year-month column and compute trends
df["year_month"] = df["parsed_date"].dt.to_period("M")
tone_trend = df.groupby("year_month")["tone"].mean().reset_index()
tone_trend["year_month"] = pd.to_datetime(tone_trend["year_month"].astype(str))
tone_trend["rolling_avg"] = tone_trend["tone"].rolling(window=3, center=True).mean()

# Plot tone trend
plt.figure(figsize=(14, 6))
plt.plot(tone_trend["year_month"], tone_trend["tone"], alpha=0.3, label='Monthly Average')
plt.plot(tone_trend["year_month"], tone_trend["rolling_avg"], color='red', label='3-Month Rolling Avg', linewidth=2)
plt.title("Average Tone Over Time (Monthly, with Smoothing)", fontsize=14)
plt.xlabel("Month")
plt.ylabel("Average Tone")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

```

The monthly average shows short-term tone shifts, while the 3-month rolling average smooths out volatility and reveals broader sentiment trends.

## Election Influence on Media Tone

U.S. elections can shape media tone dramatically due to increased political coverage and polarization.

2016 Presidential: Highly divisive race may have driven negative coverage.

2018 Midterms: Conflict-heavy narratives around congressional power shifts.

2020 Presidential: COVID, mail-in voting, and election denial dominated tone.

2022 Midterms: Continued division and debate over policy issues.

2024 Presidential: Early campaign portrayals of Trump likely influenced tone.

## COVID Influence

```{python}

import matplotlib.dates as mdates

# Mark key U.S. elections
election_events = {
    "2016 Presidential": "2016-11-08",
    "2018 Midterms": "2018-11-06",
    "2020 Presidential": "2020-11-03",
    "2022 Midterms": "2022-11-08",
    "2024 Presidential": "2024-11-05",
    "COVID": "2020-03-10"
}
event_dates = {label: pd.to_datetime(date) for label, date in election_events.items()}

# Plot with election overlays
plt.figure(figsize=(14, 6))
plt.plot(tone_trend["year_month"], tone_trend["tone"], alpha=0.3, label='Monthly Average')
plt.plot(tone_trend["year_month"], tone_trend["rolling_avg"], color='red', label='3-Month Rolling Avg', linewidth=2)

# Draw election lines
for label, date in event_dates.items():
    plt.axvline(date, color='blue', linestyle='--', alpha=0.7)
    plt.text(date, tone_trend["tone"].min() + 0.2, label, rotation=90, verticalalignment='bottom', fontsize=8)

plt.title("Average Tone Over Time (Monthly, with Elections Overlayed)", fontsize=14)
plt.xlabel("Month")
plt.ylabel("Average Tone")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

```

###

```{python}
import pandas as pd

# Ensure parsed_date is datetime and timezone-naive
df["parsed_date"] = pd.to_datetime(df["parsed_date"], errors="coerce").dt.tz_localize(None)

# Define U.S. election dates
election_dates = {
    "2016 Presidential": pd.to_datetime("2016-11-08"),
    "2018 Midterms": pd.to_datetime("2018-11-06"),
    "2020 Presidential": pd.to_datetime("2020-11-03"),
    "2022 Midterms": pd.to_datetime("2022-11-08"),
    "2024 Presidential": pd.to_datetime("2024-11-05")
}

# Analyze tone before vs. after each election
results = []
for label, date in election_dates.items():
    pre = df[(df["parsed_date"] >= date - pd.DateOffset(months=3)) & (df["parsed_date"] < date)]
    post = df[(df["parsed_date"] >= date) & (df["parsed_date"] < date + pd.DateOffset(months=3))]

    results.append({
        "election": label,
        "pre_avg_tone": pre["tone"].mean(),
        "post_avg_tone": post["tone"].mean(),
        "tone_shift": post["tone"].mean() - pre["tone"].mean()
    })

# Create results DataFrame
tone_shift_df = pd.DataFrame(results)
print(tone_shift_df)

```

```{python}

import matplotlib.pyplot as plt
import numpy as np

# Setup for bar plot
labels = tone_shift_df["election"]
x = np.arange(len(labels))
width = 0.35

plt.figure(figsize=(10, 6))
bars1 = plt.bar(x - width/2, tone_shift_df["pre_avg_tone"], width, label='3 Months Before')
bars2 = plt.bar(x + width/2, tone_shift_df["post_avg_tone"], width, label='3 Months After')

plt.ylabel("Average Tone")
plt.title("Average Tone Before vs. After U.S. Elections")
plt.xticks(x, labels, rotation=45, ha="right")
plt.axhline(0, color='black', linewidth=0.5)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.5)

# Annotate tone shift on top
for i in range(len(x)):
    shift = tone_shift_df["tone_shift"].iloc[i]
    plt.text(x[i], max(tone_shift_df["pre_avg_tone"].iloc[i], tone_shift_df["post_avg_tone"].iloc[i]) + 0.1,
             f"+{shift:.2f}" if shift > 0 else f"{shift:.2f}", 
             ha='center', fontsize=9, color='green' if shift > 0 else 'red')

plt.tight_layout()
plt.show()

```

The tone values in GDELT typically range from â€“10 to +10, but most news content clusters between â€“5 and +1.

Your tone mean is around â€“2.7, so a +0.3 shift is a ~11% relative change, which is meaningful in sentiment analysis terms â€” especially across tens of thousands of articles.

All 5 elections showed a positive shift â€” even if modest, that pattern suggests a reliable directional trend, not random fluctuation.

```{python}

from scipy.stats import ttest_ind

for label, date in election_dates.items():
    pre = df[(df["parsed_date"] >= date - pd.DateOffset(months=3)) & (df["parsed_date"] < date)]["tone"].dropna()
    post = df[(df["parsed_date"] >= date) & (df["parsed_date"] < date + pd.DateOffset(months=3))]["tone"].dropna()
    
    t_stat, p_val = ttest_ind(post, pre, equal_var=False)
    print(f"{label}: p = {p_val:.4f}")

```

```{python}

from collections import Counter

# Drop missing themes and split by semicolon
themes_series = df["V2Themes"].dropna().str.split(";")

# Flatten the list of all theme entries
all_themes = [theme.split(",")[0] for sublist in themes_series for theme in sublist if theme]

# Count the most frequent themes
theme_counts = Counter(all_themes).most_common(20)

# Print top themes
print("Top 20 Most Common Themes:")
for theme, count in theme_counts:
    print(f"{theme}: {count}")

```

###

```{python}
import pandas as pd

election_dates = {
    "2016 Presidential": pd.Timestamp("2016-11-08"),
    "2018 Midterms": pd.Timestamp("2018-11-06"),
    "2020 Presidential": pd.Timestamp("2020-11-03"),
    "2022 Midterms": pd.Timestamp("2022-11-08"),
    "2024 Presidential": pd.Timestamp("2024-11-05"),
}
```

```{python}

from collections import Counter

# Function to get theme counts in a specific date range
def get_theme_counts(start_date, end_date):
    mask = (df["parsed_date"] >= start_date) & (df["parsed_date"] <= end_date)
    themes_series = df.loc[mask, "V2Themes"].dropna().str.split(";")
    all_themes = [theme.split(",")[0] for sublist in themes_series for theme in sublist if theme]
    return Counter(all_themes)

# Analyze themes before and after each election
theme_shift_analysis = {}

for election, date in election_dates.items():
    pre_start = date - pd.DateOffset(months=3)
    pre_end = date - pd.DateOffset(days=1)
    post_start = date + pd.DateOffset(days=1)
    post_end = date + pd.DateOffset(months=3)

    pre_counts = get_theme_counts(pre_start, pre_end)
    post_counts = get_theme_counts(post_start, post_end)

    # Calculate the difference in theme frequencies
    theme_diff = {theme: post_counts[theme] - pre_counts.get(theme, 0) for theme in post_counts}

    # Sort themes by the magnitude of change
    sorted_theme_diff = sorted(theme_diff.items(), key=lambda item: abs(item[1]), reverse=True)

    theme_shift_analysis[election] = sorted_theme_diff[:10]  # Top 10 themes with the most change

# Display the results
for election, changes in theme_shift_analysis.items():
    print(f"\nTop Theme Changes for {election}:")
    for theme, change in changes:
        print(f"{theme}: {'+' if change > 0 else ''}{change}")


```

```{python}
# Full theme mapping from your earlier pasted output with cleaner names
theme_name_mapping = {
    "LEADER": "Leaders",
    "TAX_FNCACT_PRESIDENT": "Presidents",
    "USPEC_POLITICS_GENERAL1": "General Politics",
    "IMMIGRATION": "Immigration",
    "WB_2769_JOBS_STRATEGIES": "Job Strategies",
    "WB_2837_IMMIGRATION": "Immigration (WB)",
    "WB_2836_MIGRATION_POLICIES_AND_JOBS": "Migration Policies",
    "WB_2670_JOBS": "Jobs",
    "EPU_CATS_MIGRATION_FEAR_MIGRATION": "Migration Fear",
    "GENERAL_GOVERNMENT": "Government",
    "BORDER": "Border",
    "CRISISLEX_CRISISLEXREC": "Crisis Reporting",
    "NATURAL_DISASTER_HURRICANE": "Hurricanes",
    "TAX_WORLDMAMMALS_FOX": "Fox News (Self)",
    "EPU_POLICY_GOVERNMENT": "Government Policy",
    "TAX_FNCACT_POLICE": "Police",
    "UNGP_CRIME_VIOLENCE": "Crime & Violence",
    "HEALTH_VACCINATION": "Vaccination",
    "WB_639_REPRODUCTIVE_MATERNAL_AND_CHILD_HEALTH": "Reproductive & Child Health",
    "WB_642_CHILD_HEALTH": "Child Health",
    "WB_1459_IMMUNIZATIONS": "Immunizations",
    "UNGP_HEALTHCARE": "Healthcare (UNGP)",
    "TAX_FNCACT_NOMINEE": "Nominees",
    "MEDIA_SOCIAL": "Social Media",
    "ELECTION": "Election",
    "ECON_INFLATION": "Inflation",
    "WB_1104_MACROECONOMIC_VULNERABILITY_AND_DEBT": "Macro Vulnerability & Debt",
    "WB_442_INFLATION": "Inflation (WB)",
    "TAX_POLITICAL_PARTY_DEMOCRATS": "Democrats",
    "TAX_FNCACT_QUEEN": "Queen",
    "TAX_FNCACT_VICE_PRESIDENT": "Vice Presidents",
    "CRISISLEX_C07_SAFETY": "Safety",
    "MANMADE_DISASTER_IMPLIED": "Manmade Disaster",
    "WB_2432_FRAGILITY_CONFLICT_AND_VIOLENCE": "Conflict & Fragility"
}

# Apply renaming to your theme DataFrame
theme_df["Theme"] = theme_df["Theme"].replace(theme_name_mapping)

# Show cleaned theme list
theme_df.head(10)
```

```{python}

import matplotlib.pyplot as plt
import seaborn as sns

# Reuse existing cleaned `theme_df` and plot the heatmap without redefining themes
pivot_df = theme_df.pivot(index="Theme", columns="Election", values="Tone Shift").fillna(0)

plt.figure(figsize=(12, 10))
sns.heatmap(pivot_df, cmap="RdBu_r", center=0, annot=True, fmt=".0f", linewidths=0.5)
plt.title("Theme Tone Shift Across Elections")
plt.ylabel("Theme")
plt.xlabel("Election")
plt.tight_layout()
plt.show()

```

```{python}

# Create a separate heatmap for each election from the existing cleaned theme_df

# Loop through each unique election and plot individual heatmaps
unique_elections = theme_df["Election"].unique()

for election in unique_elections:
    single_df = theme_df[theme_df["Election"] == election].pivot(index="Theme", columns="Election", values="Tone Shift").fillna(0)
    
    plt.figure(figsize=(6, 10))
    sns.heatmap(single_df, cmap="RdBu_r", center=0, annot=True, fmt=".0f", linewidths=0.5)
    plt.title(f"Theme Tone Shift â€“ {election}")
    plt.xlabel("Election")
    plt.ylabel("Theme")
    plt.tight_layout()
    plt.show()


```